{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on movie comments\n",
    "The data we're using are movie comments from [IMDB](http://www.imdb.com/). It has been preprocessed and stored in `reviews.txt` and `labels.txt`. For the same line in these two files, `reviews.txt` contains the review and `labels.txt` contains corresponding label which is either `positive` or `negative`. The goal is to train a model against this data to tell whether a comment is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Preparing data\n",
    "Task for this part:  \n",
    "1. Convert a review to a vector (it's much easier to work with the neural network)\n",
    "2. Reduce noise in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "The data has been preprocessed that it only contains lower case characters. Labels are converted into upper case here to make it more like a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('reviews.txt', 'r') as f:\n",
    "    reviews = list(map(lambda x : x[:-1], f.readlines()))\n",
    "\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = list(map(lambda x : x[:-1].upper(), f.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count word frequency\n",
    "The count of a word in all reviews can provide some useful information:\n",
    "1. A word appears more often in positive reviews is found in a review, it's more likely to a positive review (same for negative)\n",
    "2. A word appears equally often in positive and negative reviews contribute very little to the prediction\n",
    "\n",
    "The first step is to count word frequency and then the count will be used to create a vocabulary to encode the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()\n",
    "\n",
    "for review, label in zip(reviews, labels):\n",
    "    for word in review.split(' '):\n",
    "        total_counts[word] += 1\n",
    "        if label == 'POSITIVE':\n",
    "            positive_counts[word] += 1\n",
    "        if label == 'NEGATIVE':\n",
    "            negative_counts[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ratios of positive and negative uses of words, notice the +1 in the denominator â€“ that ensures we don't divide by zero for words that are only seen in positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "for word in total_counts.keys():\n",
    "    pos_neg_ratios[word] = positive_counts[word] / float(negative_counts[word]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio values are not easy to compare for two reasons:\n",
    "1. Neutral value is 1 instead of 0, comparing absolute values are easier when around zero\n",
    "2. Ratios of positive and negative words don't have same magnitude  \n",
    "\n",
    "To solve these two problems, convert ratios to logs in such a way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, ratio in pos_neg_ratios.items():\n",
    "    if ratio >= 1:\n",
    "        pos_neg_ratios[word] = np.log(ratio)\n",
    "    else:\n",
    "        pos_neg_ratios[word] = -np.log(1/(ratio+0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Reduction\n",
    "There are two kinds of noise we want to reduce:\n",
    "1. Words appearing equally often in positive and negative reviews, they contribute very little to the prediction\n",
    "2. Words appearing seldomly, they could be self made and not representational\n",
    "\n",
    "We use two parameters `min_count` and `polarity_cutoff` to filter out those words and add the rest into vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 50\n",
    "polarity_cutoff = 0.1\n",
    "\n",
    "vocab = set()\n",
    "for word, count in total_counts.items():\n",
    "    if count > min_count and abs(pos_neg_ratios[word]) > polarity_cutoff:\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the word-to-index dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict() \n",
    "for i, word in enumerate(vocab):\n",
    "    word2idx[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a review into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_vector(review):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    for word in review.split(' '):\n",
    "        idx = word2idx.get(word, None)\n",
    "        if idx:\n",
    "            vector[idx] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run through the entire review data set and convert each review to a word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_vectors = np.array([review_to_vector(r) for r in reviews])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validation and Test sets\n",
    "Here we're using the function to_categorical from TFLearn to reshape the target data so that we'll have two output units and can classify with a softmax activation function. We actually won't be creating the validation set here, TFLearn will do that for us later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical\n",
    "\n",
    "Y = (np.array(labels) == 'POSITIVE').astype(np.int_)\n",
    "records = len(labels)\n",
    "\n",
    "shuffle = np.arange(records)\n",
    "np.random.shuffle(shuffle)\n",
    "test_fraction = 0.9\n",
    "\n",
    "train_split, test_split = shuffle[:int(records*test_fraction)], shuffle[int(records*test_fraction):]\n",
    "trainX, trainY = review_vectors[train_split,:], to_categorical(Y[train_split], 2)\n",
    "testX, testY = review_vectors[test_split,:], to_categorical(Y[test_split], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Build the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use [TFlearn](http://tflearn.org/) to build the network layer by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # This resets all parameters and variables, leave this here\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    net = tflearn.input_data([None, len(vocab)])\n",
    "    net = tflearn.fully_connected(net,150, activation='ReLU')\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    net = tflearn.regression(net, optimizer='sgd', learning_rate=0.01, loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tflearn.DNN(net)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 14309  | total loss: \u001b[1m\u001b[32m0.14391\u001b[0m\u001b[0m | time: 3.732s\n",
      "| SGD | epoch: 090 | loss: 0.14391 - acc: 0.9521 -- iter: 20224/20250\n",
      "Training Step: 14310  | total loss: \u001b[1m\u001b[32m0.14102\u001b[0m\u001b[0m | time: 4.759s\n",
      "| SGD | epoch: 090 | loss: 0.14102 - acc: 0.9538 | val_loss: 0.23216 - val_acc: 0.9049 -- iter: 20250/20250\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "# Training\n",
    "model.fit(trainX, trainY, validation_set=0.1, show_metric=True, batch_size=128, n_epoch=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.8864\n"
     ]
    }
   ],
   "source": [
    "predictions = (np.array(model.predict(testX))[:,0] >= 0.5).astype(np.int_)\n",
    "test_accuracy = np.mean(predictions == testY[:,0], axis=0)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try my own comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function that uses trained model to predict sentiment\n",
    "def test_review(review):\n",
    "    positive_prob = model.predict([review_to_vector(review.lower())])[0][1]\n",
    "    print('Review: {}'.format(review))\n",
    "    print('P(positive) = {:.3f} :'.format(positive_prob), \n",
    "          'Positive' if positive_prob > 0.5 else 'Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: GANTZ:O is by far the best CG movie of 2016.\n",
      "P(positive) = 0.585 : Positive\n",
      "Review: It's amazing anyone could be talented enough to make something this spectacularly awful\n",
      "P(positive) = 0.233 : Negative\n"
     ]
    }
   ],
   "source": [
    "review = \"GANTZ:O is by far the best CG movie of 2016.\"\n",
    "test_review(review)\n",
    "\n",
    "review = \"It's amazing anyone could be talented enough to make something this spectacularly awful\"\n",
    "test_review(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
